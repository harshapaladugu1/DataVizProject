{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df968141",
   "metadata": {},
   "source": [
    "## Project 1 - Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df71f7ee",
   "metadata": {},
   "source": [
    "## Team Info \n",
    "\n",
    "Project Title: Credit Card Fraud Detection\n",
    "\n",
    "Team Name: FraudBusters\n",
    "\n",
    "Team Members: \n",
    "    Sanju Kanumuri (sanju), \n",
    "    Emanda Seifu (emandats),\n",
    "    Brianna Detter (bdetter),\n",
    "    Rahul Ramakrishnan (rramakrishnan106), &\n",
    "    Harsha Paladugu (harshanba34)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50258106",
   "metadata": {},
   "source": [
    "## Project Introduction\n",
    "\n",
    "Project topic: Digital fraud has been one of the biggest illegal markets in recent times. From black market websites selling credit card information to fishing messages trying to access your personal information, financial fraud is a big issue that currently needs to be tackled. Our project will be looking over a Kaggle database that presents different features of online transactions. This training data has a column with a boolean value of 0 or 1 telling us whether or not the transaction is fraudulent. With this information, we plan on finding trends within the data that tell us what factors most contribute to fraud.\n",
    "\n",
    "\n",
    "Potential research questions you plan to address: \n",
    "What are indicators of credit card fraud attempts? \n",
    "What spending patterns that indicate fraud?  \n",
    "\n",
    "Description of potential source data: \n",
    "\"We present a synthetic dataset generated using the simulator called PaySim as an approach to such a problem. PaySim uses aggregated data from the private dataset to generate a synthetic dataset that resembles the normal operation of transactions and injects malicious behaviour to later evaluate the performance of fraud detection methods.\"\n",
    "https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\n",
    "https://www.kaggle.com/datasets/ealaxi/paysim1\n",
    "\n",
    "\n",
    "Using the information above, we plan on plotting the data, finding a regression line, finding patterns with fraudulant and nonfrrudaulant data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552e16fc",
   "metadata": {},
   "source": [
    "## Question \n",
    "What factors contribute to a transcation being flagged as fraud? \n",
    "Moreover, how can we visualize the data to best understand the most important factors in detecting fraud?\n",
    "\n",
    "#### Motivation:\n",
    "\n",
    "As we have mentioned above, digital fraud has continued to increase over the past several years and can be cited as one of the ways that illegal markets/groups will target an individual's personal information. Based on the data from Kaggle, we want to find out what different aspects of online transactions that be linked to a transcation being flagged as fraud. \n",
    "\n",
    "#### Methods:\n",
    "\n",
    "One way we are planning to research this is using Multi-dimensional Scaling (MDS) with a normalized data set in order to get a better understanding for the relationship between different features and what transactions are marked as fraud. In addition to this, we also plan to use other tools such as pie charts, bar graphs, etc. to provide readers with several varying visuals of the information provided by the Kaggle database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f169d193",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "### Data\n",
    "The dataset on Kaggle, titled \"Synthetic Financial Datasets For Fraud Detection,\" consists of a synthetic dataset generated by the PaySim mobile money simulator. This simulator mimics the transactional patterns of real-world financial systems at scale, primarily designed to provide data for research into fraud detection methods. The dataset includes several types of transactions, and it is labeled with indicators of fraudulent activity, making it a valuable resource for developing and testing fraud detection algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa71641",
   "metadata": {},
   "source": [
    "### Data Processing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bed14cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from datetime import datetime\n",
    "# from geopy.distance import geodesic\n",
    "# from geopy.geocoders import Nominatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb525724",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"PS_20174392719_1491204439457_log.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b6ff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n",
    "df.info()\n",
    "# print(len(df[df['type'].unique()]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e07539f",
   "metadata": {},
   "source": [
    "###  Visualization of Dataset \n",
    "We formed visualizations to detect or understand patterns within credit card fraud. By comparing the distributions of transaction amounts between fraudulent and non-fraudulent transactions, analysts can identify anomalies or patterns characteristic of fraud. Moreover, understanding the distribution of payment types assists in recognizing which channels might be more susceptible to fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9071d213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data to plot\n",
    "labels = 'PAYMENT', 'TRANSFER', 'CASH_OUT', 'DEBIT' ,  \"CASH_IN\"\n",
    "sizes = [len(df[df['type']==\"PAYMENT\"]), len(df[df['type']==\"TRANSFER\"]), len(df[df['type']==\"CASH_OUT\"]),  len(df[df['type']==\"DEBIT\"]),  len(df[df['type']==\"CASH_IN\"])]\n",
    "colors = ['gold', 'yellowgreen', 'lightcoral', 'lightskyblue','silver']\n",
    "explode = (0.1, 0.1, 0.1, 0.1,0.1)  # explode 1st slice\n",
    "\n",
    "# Plot\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cafa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data to plot\n",
    "labels_bar = 'Not Fraud', 'Fraud'\n",
    "heights_bar = [len(df[df['isFraud']==0]), len(df[df['isFraud']==1])]\n",
    "colors_bar = ['yellowgreen', 'red']\n",
    "explode_bar = (0.1, 0.1)\n",
    "\n",
    "# Plot\n",
    "plt.pie(heights_bar, explode=explode_bar, labels=labels_bar, colors=colors_bar,\n",
    "    autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace1a7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonfraud_data = df[df['isFraud'] == 0]\n",
    "plt.hist(nonfraud_data.amount, color='yellowgreen', edgecolor='w')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel(\"Amount of Money Taken Out\")\n",
    "plt.ylabel(\"Number of Transactions\")\n",
    "plt.title(\"Amount of Money Taken for Non-Fraudulent Transactions\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d3dd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_data = df[df['isFraud'] == 1]\n",
    "plt.hist(fraud_data.amount, color='red', edgecolor='w')\n",
    "\n",
    "plt.xlabel(\"Amount of Money Taken Out\")\n",
    "plt.ylabel(\"Number of Transactions\")\n",
    "plt.title(\"Amount of Money Taken for Fraudulent Transactions\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9e7815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link to what I was looking at:\n",
    "# https://github.com/mGalarnyk/Python_Tutorials/blob/master/Sklearn/KNN/KNN.ipynb\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# for scaling data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "x = df.drop(['isFraud', 'isFlaggedFraud', 'type', 'nameOrig', 'nameDest'], axis = 1)\n",
    "y = df['isFraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c42ae90",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x,\n",
    "                                                    y,\n",
    "                                                    random_state = 0,\n",
    "                                                    test_size = .2)\n",
    "\n",
    "# Reduce dimension to 2 with PCA\n",
    "std_clf = make_pipeline(StandardScaler(),\n",
    "                        PCA(n_components=2, random_state=0),\n",
    "                        KNeighborsClassifier(n_neighbors=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed997d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_clf.fit(x_train, y_train)\n",
    "pred_test_std = std_clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b05ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nPrediction accuracy for the standardized test dataset with PCA')\n",
    "print('{:.2%}\\n'.format(metrics.accuracy_score(y_test, pred_test_std)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5285df9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract PCA from pipeline\n",
    "pca_std = std_clf.named_steps['pca']\n",
    "\n",
    "# Use PCA with scale on X_train data for visualization.\n",
    "scaler = std_clf.named_steps['standardscaler']\n",
    "x_train_std_transformed = pca_std.transform(scaler.transform(x_train))\n",
    "\n",
    "# visualize standardized  with PCA performed\n",
    "for l, c, m in zip(range(0, 2), ('blue', 'red'), ('^', 's')):\n",
    "    plt.scatter(x_train_std_transformed[y_train == l, 0],\n",
    "                x_train_std_transformed[y_train == l, 1],\n",
    "                color=c,\n",
    "                label='class %s' % l,\n",
    "                alpha=0.5,\n",
    "                marker=m\n",
    "                )\n",
    "\n",
    "plt.title('Standardized training dataset after PCA')\n",
    "plt.xlabel('1st principal component')\n",
    "plt.ylabel('2nd principal component')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ec1110",
   "metadata": {},
   "source": [
    "### Boxplot of Transaction Types and Amount Spent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d08dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.boxplot(column=['amount'], by='type')\n",
    "\n",
    "ax.set_xlabel('Transaction Type')\n",
    "ax.set_ylabel('Amount')\n",
    "\n",
    "ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa69515c",
   "metadata": {},
   "source": [
    "### Continuation of Analysis \n",
    "Based on the graphs above we have analsyized the following: \n",
    "\n",
    "Distribution of Payment Types: The first pie chart offers insight into the relative frequencies of different payment types within the dataset. It's a helpful visualization for understanding which transaction types are most common. In the simulated data, each payment type is represented proportionally, showcasing how diverse the transaction types are within the dataset.\n",
    "\n",
    "Distribution of Fraud vs Non-Fraud: This second pie chart represents the distribution between fraudulent and non-fraudulent activities within a dataset. It shows a very small sliver, 0.1%, of the data being attributed to fraud, while the overwhelming majority, 99.9%, is not fraud. This visualization highlights the typical imbalance in datasets concerning fraudulent transactions where fraudulent activities are a very small minority. \n",
    "\n",
    "Distribution of Transaction Amounts (Non-Fraudulent): The first  histogram for non-fraudulent transactions displays the frequency distribution of transaction amounts. Typically, such a distribution helps identify common transaction sizes and how they cluster around certain values. The skewed nature of the distribution suggests that lower transaction amounts are more common, with fewer high-value transactions.\n",
    "\n",
    "Distribution of Transaction Amounts (Fraudulent): Similarly, this second histogram for fraudulent transactions highlights the distribution of transaction amounts specifically within fraudulent activities. Comparing this graph to the non-fraudulent transactions' histogram could reveal differences in transaction behaviors between fraudulent and legitimate activities. The distribution is again skewed, indicating that fraud attempts might also focus on lower value transactions, but the tail might extend further, suggesting attempts at higher value frauds as well.\n",
    "\n",
    "The graph titled \"Standardized training dataset after PCA\" illustrates the first two principal components derived from a PCA of the standardized training data. These components, represented along the axes of the graph, highlight the directions of maximum variance in the data, often revealing the underlying structure. Points on the graph, possibly color-coded to differentiate classes, show how well PCA has managed to segregate different classes, such as fraud versus non-fraud transactions. Effective separation in this 2D space suggests that these components capture essential distinctions between classes. This visualization not only simplifies data analysis by reducing dimensionality but also aids in understanding which features significantly influence these principal components.\n",
    "\n",
    "Boxplot of Transaction Types and Amount Spent: While this information is also conveyed in some other formats, this box and whisker plot highlights which payment types are most used within this data set. According to the information above, transfers account for a majority of the payment amounts. The second most used is cash out, followed by cash in then debit at last position along with payment. \n",
    "\n",
    "This data is important to visualize as it can give an idea for preventative measures that should be taken to reduce fraud. Since transfers are the largest payment types, adding more security procedures such as more checkpoints before allowing for money to be transferred from one account to another. In addition to this, adding questions to ensure the money is transferred to the correct individual/group would reduce and minimize the potential opportunities for fraud. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d261527",
   "metadata": {},
   "source": [
    "### Correlation Analysis \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d91b48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the numerical columns for correlation\n",
    "numerical_df = df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = numerical_df.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix of Variables\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d75d41d",
   "metadata": {},
   "source": [
    "The heatmap analysis of our dataset has shed light on the relationships between transactional features and fraudulent activity. The results reveal a notable absence of strong linear correlations among the majority of numerical features and instances of fraud, indicated by isFraud. This suggests that the characteristics of fraudulent transactions are not directly related to the measured variables, such as the transaction amount, account balances before and after transactions, or the transaction step time. Furthermore, the disconnection between isFlaggedFraud and isFraud suggests a misalignment in the current mechanisms used to flag potential fraud. Consequently, this necessitates a shift in our project strategy towards more sophisticated analytical techniques. The next phase will explore the use of complex algorithms that can detect subtle patterns and interactions beyond the reach of simple linear models. We will delve into multi-dimensional scaling and anomaly detection and consider enhancing our feature set to better characterize fraudulent behavior. Additionally, the categorical variables warrant further examination through encoding techniques and cluster analysis. These efforts aim to reveal the elusive patterns of digital fraud and contribute to the development of robust detection systems essential for modern finance. As we embrace a multi-faceted analytical approach, we stand to deepen our understanding of fraud detection in the digital domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21031ac",
   "metadata": {},
   "source": [
    "## Follow Up Questions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7c374d",
   "metadata": {},
   "source": [
    "In light of the findings thus far, our project stands at the cusp of further discovery, prompting us to consider additional avenues of inquiry to better decipher the complexities of credit card fraud detection:\n",
    "\n",
    "1. **Categorical Dynamics**: With the transaction type being a categorical variable, how might its various categories correlate with fraudulent occurrences? The exploration of encoding these categorical variables could offer a clearer depiction of their influence.\n",
    "\n",
    "2. **Chronological Fluctuations**: Does the variable 'step', indicative of time, play a role in the frequency of fraud? Identifying patterns over time could shed light on whether fraud attempts are concentrated within specific intervals.\n",
    "\n",
    "3. **Detection Sensitivity**: Considering the rarity of fraudulent transactions, what methods can we employ to enhance our model's sensitivity to detect such scarce events? Are there alternative analytical strategies or specific performance metrics that could improve detection accuracy?\n",
    "\n",
    "4. **Algorithm Efficacy**: In the quest to identify the most proficient machine learning algorithms for our purpose, we must ask— which models excel in capturing the intricate patterns of fraud within our dataset? How do these models fare against one another in terms of their predictive prowess?\n",
    "\n",
    "5. **Innovative Feature Creation**: What innovative features could be engineered to serve as more potent indicators of fraudulent activity? Could certain derived ratios or differences between balances provide a more telling signal?\n",
    "\n",
    "6. **Flagging System Refinement**: The criteria for transactions being flagged as fraudulent merit reevaluation. How can we enhance the flagging mechanism to better align with genuine instances of fraud based on our analytical insights?\n",
    "\n",
    "7. **Real-World Application**: How will our model stand the test against real-world data? The integration of external datasets and factors might be imperative in refining our predictive capabilities.\n",
    "\n",
    "8. **Preventive Measures**: Finally, how can we translate our analytical conclusions into actionable measures for real-time fraud prevention in digital financial ecosystems?\n",
    "\n",
    "Addressing these queries is the logical progression of our research endeavor. As we further this scholarly pursuit, our collective aim is to sculpt a comprehensive and dynamic approach to combating digital financial fraud, bolstering the defenses of the financial domain against such nefarious activities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0097d955",
   "metadata": {},
   "source": [
    "## Previous Follow Up Questions Answered\n",
    "\n",
    "5. **Innovative Feature Creation**: Develop features that might indicate fraud more clearly, such as the ratio of transaction amount to customer average transaction amount, or new variables reflecting recent account activity (e.g., number of transactions in the last hour). These engineered features can provide deeper insights and potentially improve detection accuracy.\n",
    "\n",
    "6. **Flagging System Refinement**: Reevaluate the criteria used for flagging transactions by analyzing false positives and false negatives. Adjust thresholds and rules based on the insights gained from data analysis and feature importance scores derived from machine learning models.\n",
    "\n",
    "7. **Real-World Application**: Test the model with real-world data, possibly through a pilot within a controlled environment. Integrate external factors like IP address geolocation, device fingerprinting, and customer behavior patterns from other databases to enhance the model's robustness and accuracy.\n",
    "\n",
    "8. **Preventive Measures**: Translate findings into actionable strategies, such as setting up real-time alerts, adjusting customer transaction limits based on risk levels, and developing customer profiles that help in early detection of anomalies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d01139",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5180a32b",
   "metadata": {},
   "source": [
    "In conclusion, this phase of the project has allowed us to delve deeper into the correlations between fraud and digital transactions. After discussing possible approaches for analyzing the data, we decided to use multi-dimensional scaling (MDS) after normalizing the data provided from the Kaggle database. Using the updated data, we created graphs to highlight different factors that play a role in digital fraud via visual representation. The analysis provides more information about each chart. In phase 3 of this project, we will continue to analyze the \n",
    "relationship between digital transactions and said transactions being marked as fraud. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d7c981",
   "metadata": {},
   "source": [
    "## Team Member Contribution \n",
    "#### Sanju - Answered follow-up questions and helped with analysis \n",
    "#### Emanda -  Created box and whisker plot, worked on the analysis \n",
    "#### Brianna - \n",
    "#### Harsha - \n",
    "#### Rahul - "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
